{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\python39\\lib\\site-packages (4.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK -- Connected to MongoDB at server localhost\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "MONGODB_HOST = 'localhost'\n",
    "MONGODB_PORT = '27017'\n",
    "MONGODB_TIMEOUT = 1000\n",
    "\n",
    "URI_CONNECTION = \"mongodb://\" + MONGODB_HOST + \":\" + MONGODB_PORT +  \"/\"\n",
    "\n",
    "try:\n",
    "     client = pymongo.MongoClient(URI_CONNECTION)\n",
    "     client.server_info()\n",
    "     print ('OK -- Connected to MongoDB at server %s' % (MONGODB_HOST))\n",
    "     client.close()\n",
    "except pymongo.errors.ServerSelectionTimeoutError as error:\n",
    "        print ('Error with MongoDB connection: %s' % error)\n",
    "except pymongo.errors.ConnectionFailure as error:\n",
    "        print ('Could not connect to MongoDB: %s' % error)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Scrapy in c:\\python39\\lib\\site-packages (2.6.2)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\python39\\lib\\site-packages (from Scrapy) (2.0.1)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\python39\\lib\\site-packages (from Scrapy) (1.0.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Twisted>=17.9.0 in c:\\python39\\lib\\site-packages (from Scrapy) (22.8.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\python39\\lib\\site-packages (from Scrapy) (22.0.0)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in c:\\python39\\lib\\site-packages (from Scrapy) (21.1.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\python39\\lib\\site-packages (from Scrapy) (4.9.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\python39\\lib\\site-packages (from Scrapy) (2.0.6)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\python39\\lib\\site-packages (from Scrapy) (1.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\python39\\lib\\site-packages (from Scrapy) (62.3.2)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\python39\\lib\\site-packages (from Scrapy) (1.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\python39\\lib\\site-packages (from Scrapy) (5.4.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\python39\\lib\\site-packages (from Scrapy) (1.6.2)\n",
      "Requirement already satisfied: tldextract in c:\\python39\\lib\\site-packages (from Scrapy) (3.3.1)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\python39\\lib\\site-packages (from Scrapy) (37.0.4)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\python39\\lib\\site-packages (from Scrapy) (0.7.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\python39\\lib\\site-packages (from Scrapy) (0.2.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\python39\\lib\\site-packages (from cryptography>=2.0->Scrapy) (1.15.1)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\python39\\lib\\site-packages (from itemloaders>=1.0.1->Scrapy) (1.0.1)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\python39\\lib\\site-packages (from parsel>=1.5.0->Scrapy) (1.16.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\python39\\lib\\site-packages (from service-identity>=16.0.0->Scrapy) (0.4.8)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\python39\\lib\\site-packages (from service-identity>=16.0.0->Scrapy) (21.4.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\python39\\lib\\site-packages (from service-identity>=16.0.0->Scrapy) (0.2.8)\n",
      "Requirement already satisfied: incremental>=21.3.0 in c:\\python39\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (21.3.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\python39\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (15.1.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\python39\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (21.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\python39\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (4.3.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in c:\\python39\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (20.2.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\python39\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\python39\\lib\\site-packages (from tldextract->Scrapy) (3.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\python39\\lib\\site-packages (from tldextract->Scrapy) (1.5.1)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\python39\\lib\\site-packages (from tldextract->Scrapy) (2.28.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\k.key b\\appdata\\roaming\\python\\python39\\site-packages (from tldextract->Scrapy) (3.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\python39\\lib\\site-packages (from cffi>=1.12->cryptography>=2.0->Scrapy) (2.21)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python39\\lib\\site-packages (from requests>=2.1.0->tldextract->Scrapy) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python39\\lib\\site-packages (from requests>=2.1.0->tldextract->Scrapy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python39\\lib\\site-packages (from requests>=2.1.0->tldextract->Scrapy) (2022.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement beatifulsoup4 (from versions: none)\n",
      "ERROR: No matching distribution found for beatifulsoup4\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install Scrapy \n",
    "%pip install beatifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.item import Field\n",
    "from scrapy.item import Item\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.loader.processors import MapCompose\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.loader import ItemLoader\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import logging\n",
    "from scrapy.utils.project import get_project_settings\n",
    "import logging\n",
    "\n",
    "class Articulo(Item):\n",
    "    titulo = Field()\n",
    "    precio = Field()\n",
    "    descripcion = Field()\n",
    "\n",
    "class daneCrawler(CrawlSpider):\n",
    "    name = 'dane'\n",
    "\n",
    "    custom_settings = {\n",
    "      'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/71.0.3578.80 Chrome/71.0.3578.80 Safari/537.36',\n",
    "      'CLOSESPIDER_PAGECOUNT': 20 # Numero maximo de paginas en las cuales voy a descargar items. Scrapy se cierra cuando alcanza este numero\n",
    "    }\n",
    "\n",
    "    # Utilizamos 2 dominios permitidos, ya que los articulos utilizan un dominio diferente\n",
    "    allowed_domains = ['dane.gov.co', 'www.dane.gov.co']\n",
    "\n",
    "    start_urls = ['https://www.dane.gov.co/index.php/estadisticas-por-tema/industria/encuesta-anual-manufacturera-enam/eam-historicos']\n",
    "\n",
    "    download_delay = 1\n",
    "\n",
    "    # Tupla de reglas\n",
    "    rules = (\n",
    "        Rule( # REGLA #1 => HORIZONTALIDAD POR PAGINACION\n",
    "            LinkExtractor(\n",
    "                allow=r'/_Desde_\\d+' # Patron en donde se utiliza \"\\d+\", expresion que puede tomar el valor de cualquier combinacion de numeros\n",
    "            ), follow=True),\n",
    "        Rule( # REGLA #2 => VERTICALIDAD AL DETALLE DE LOS PRODUCTOS\n",
    "            LinkExtractor(\n",
    "                allow=r'/MCO' \n",
    "            ), follow=True, callback='parse_items'), # Al entrar al detalle de los productos, se llama al callback con la respuesta al requerimiento\n",
    "    )\n",
    "\n",
    "    def parse_items(self, response):\n",
    "\n",
    "        item = ItemLoader(Articulo(), response)\n",
    "            \n",
    "        item.add_xpath('Período de referencia', '/html/body/div[1]/div[3]/div/div[1]/div/article/section/table[1]/tbody/tr[5]/td[1]/span/strong', MapCompose(lambda i: i.replace('\\n', ' ').replace('\\r', ' ').strip()))\n",
    "        item.add_xpath('Anexos', '//*[@id=\"t3-content\"]/div/article/section/table[1]/tbody/tr[2]/td[4]/ul'.replace('\\r', ' ').strip())\n",
    "        \n",
    "       \n",
    "        yield item.load_item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Develop\\Python\\Mineria_datos_importaciones\\spiders\\CuadernoWebscraping.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m logging\u001b[39m.\u001b[39mbasicConfig(level\u001b[39m=\u001b[39mlogging\u001b[39m.\u001b[39mDEBUG)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m process \u001b[39m=\u001b[39m CrawlerProcess(get_project_settings())\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# CORRIENDO SCRAPY SIN LA TERMINAL\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# process = CrawlerProcess({\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#      'FEED_FORMAT': 'csv',\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#      'FEED_URI': 'datos_de_salida6.csv',\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#     'overwrite': True\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#  })\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "# CORRIENDO SCRAPY SIN LA TERMINAL\n",
    "# process = CrawlerProcess({\n",
    "#      'FEED_FORMAT': 'csv',\n",
    "#      'FEED_URI': 'datos_de_salida6.csv',\n",
    "#     'overwrite': True\n",
    "#  })\n",
    "process.crawl(daneCrawler)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.spiders import CSVFeedSpider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class DemoSpider(CSVFeedSpider): \n",
    "   name = \"demo\" \n",
    "   allowed_domains = [\"www.demoexample.com\"] \n",
    "   start_urls = [\"http://www.demoexample.com/feed.csv\"] \n",
    "   delimiter = \";\" \n",
    "   quotechar = \"'\" \n",
    "   headers = [\"product_title\", \"product_link\", \"product_description\"]  \n",
    "   \n",
    "   def parse_row(self, response, row): \n",
    "      self.logger.info(\"This is row: %r\", row)  \n",
    "      item = dict()\n",
    "      item[\"product_title\"] = row[\"product_title\"] \n",
    "      item[\"product_link\"] = row[\"product_link\"] \n",
    "      item[\"product_description\"] = row[\"product_description\"]\n",
    "\n",
    "      print(item)\n",
    "      return item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 23:56:44 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: scrapybot)\n",
      "2022-09-26 23:56:44 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.8.0, Python 3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.0.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 37.0.4, Platform Windows-10-10.0.19044-SP0\n",
      "2022-09-26 23:56:44 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2022-09-26 23:56:44 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2022-09-26 23:56:44 [scrapy.extensions.telnet] INFO: Telnet Password: eda16754829cc425\n",
      "2022-09-26 23:56:44 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-09-26 23:56:44 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-09-26 23:56:44 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-09-26 23:56:44 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-09-26 23:56:44 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-09-26 23:56:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-09-26 23:56:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32me:\\Develop\\Python\\Mineria_datos_importaciones\\spiders\\CuadernoWebscraping.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m process \u001b[39m=\u001b[39m CrawlerProcess({\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mUSER_AGENT\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mMozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     })\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m process\u001b[39m.\u001b[39mcrawl(DemoSpider)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Develop/Python/Mineria_datos_importaciones/spiders/CuadernoWebscraping.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m process\u001b[39m.\u001b[39;49mstart()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\scrapy\\crawler.py:348\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[1;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[0;32m    346\u001b[0m tp\u001b[39m.\u001b[39madjustPoolsize(maxthreads\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39mgetint(\u001b[39m'\u001b[39m\u001b[39mREACTOR_THREADPOOL_MAXSIZE\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m    347\u001b[0m reactor\u001b[39m.\u001b[39maddSystemEventTrigger(\u001b[39m'\u001b[39m\u001b[39mbefore\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mshutdown\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop)\n\u001b[1;32m--> 348\u001b[0m reactor\u001b[39m.\u001b[39;49mrun(installSignalHandlers\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\twisted\\internet\\base.py:1317\u001b[0m, in \u001b[0;36m_SignalReactorMixin.run\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, installSignalHandlers: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstartRunning(installSignalHandlers\u001b[39m=\u001b[39;49minstallSignalHandlers)\n\u001b[0;32m   1318\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmainLoop()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\twisted\\internet\\base.py:1299\u001b[0m, in \u001b[0;36m_SignalReactorMixin.startRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[39mExtend the base implementation in order to remember whether signal\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[39mhandlers should be installed later.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[39m    installed during startup.\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_installSignalHandlers \u001b[39m=\u001b[39m installSignalHandlers\n\u001b[1;32m-> 1299\u001b[0m ReactorBase\u001b[39m.\u001b[39;49mstartRunning(cast(ReactorBase, \u001b[39mself\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\twisted\\internet\\base.py:843\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    841\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mReactorAlreadyRunning()\n\u001b[0;32m    842\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_startedBefore:\n\u001b[1;32m--> 843\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mReactorNotRestartable()\n\u001b[0;32m    844\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_started \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopped \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "process.crawl(DemoSpider)\n",
    "process.start(stop_after_crawl=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
